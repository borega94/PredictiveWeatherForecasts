# -*- coding: utf-8 -*-
"""Basic DNN for Group 8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HH1xWpHnKwfy7JsbCVj9DarzCDytWdD2

# Deep Neural Network
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras

heart = pd.read_csv("/content/heart.csv")

y = heart.target
X = heart.drop(["target"], axis = 1)

X.head()

"""## OneHot Encoding"""

from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(drop='first')

categorical_columns = ["sex","cp","fbs","restecg","exang","slope","ca","thal"]
non_categorical = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']

ohe_dataset = ohe.fit_transform(heart[categorical_columns])
columns_ohe = ohe.get_feature_names(input_features = heart[categorical_columns].columns)
ohe_dataset = pd.DataFrame(ohe_dataset.toarray())
ohe_dataset.columns = columns_ohe

X_OH = pd.concat([X.drop(categorical_columns,axis = 1),ohe_dataset],axis = 1)

X_OH.head()

"""## Standardization or Normalization"""

# Standardization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

min_heart = X_OH.min()
max_heart = X_OH.max()

X_std = scaler.fit_transform(X_OH)

X_std = pd.DataFrame(X_std)

X_std.columns = X_OH.columns

# Normalization
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

min_heart = X_OH.min()
max_heart = X_OH.max()

X_normalized = scaler.fit_transform(X_OH)

X_normalized = pd.DataFrame(X_normalized)

X_normalized.columns = X_OH.columns

"""## Splitting Data"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_std, 
    y, 
    test_size=0.2,  
    random_state=629) 

X_train_tree, X_test_tree, y_train_tree, y_test_tree = train_test_split(
    X, 
    y, 
    test_size=0.2,  
    random_state=629)

"""## Deep Neural Network"""

val_acc_threshold = 0.93
acc_threshold = 0.95

class myCallback(tf.keras.callbacks.Callback): 
    def on_epoch_end(self, epoch, logs={}): 
        if((logs.get('accuracy') > acc_threshold) & (logs.get('val_accuracy') > val_acc_threshold)):    
          print("Reached  val_accuracy, so stopping training!!".format(acc_threshold))
          self.model.stop_training = True

callbacks = myCallback()

model = keras.Sequential([
                          keras.layers.Dense(128,activation = "relu"),
                          keras.layers.BatchNormalization(),
                          keras.layers.Dense(64,activation = "relu"),
                          keras.layers.BatchNormalization(),
                          keras.layers.Dense(16,activation = "relu"),
                          keras.layers.BatchNormalization(),
                          keras.layers.Dense(1,activation="sigmoid")
                          ])
## Callback
  

adam_opt = keras.optimizers.Adam(lr=0.001)
model.compile(optimizer=adam_opt,loss = 'binary_crossentropy', metrics=['accuracy'])


history = model.fit(X_train,
                    y_train,
                    epochs=300,
                    verbose=0,
                    validation_data = (X_test,y_test),
                    callbacks=[callbacks])

plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])

from sklearn import metrics

y_dnn_pred = model.predict_classes(X_test)

y_dnn_pred = np.squeeze(y_dnn_pred)

y_dnn_pred

y_dnn_prob = model.predict(X_test)

y_dnn_prob = np.squeeze(y_dnn_prob)

y_dnn_prob

print("Accuracy:",metrics.accuracy_score(y_test, y_dnn_pred))

